{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Import the libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-03T11:17:01.740557Z","iopub.status.busy":"2024-03-03T11:17:01.740018Z","iopub.status.idle":"2024-03-03T11:17:01.746473Z","shell.execute_reply":"2024-03-03T11:17:01.745368Z","shell.execute_reply.started":"2024-03-03T11:17:01.740528Z"},"id":"kfJNHJMVhaVZ","outputId":"5e20f871-f2e9-4a38-f898-1c69e8f9ff12","trusted":true},"outputs":[],"source":["import transformers\n","import json\n","import numpy as np\n","import tensorflow as tf\n","np.object = object\n","import torch\n","\n","from datasets import Dataset\n","print(transformers.__version__)"]},{"cell_type":"markdown","metadata":{"id":"L6dhLgWAhaVc"},"source":["We also quickly upload some telemetry - this tells us which examples and software versions are getting used so we know where to prioritize our maintenance efforts. We don't collect (or care about) any personally identifiable information, but if you'd prefer not to be counted, feel free to skip this step or delete this cell entirely."]},{"cell_type":"markdown","metadata":{},"source":["## Import the trained model (model, tokenizer and data collator)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-03T11:17:01.748832Z","iopub.status.busy":"2024-03-03T11:17:01.748497Z","iopub.status.idle":"2024-03-03T11:17:02.037048Z","shell.execute_reply":"2024-03-03T11:17:02.036254Z","shell.execute_reply.started":"2024-03-03T11:17:01.748802Z"},"trusted":true},"outputs":[],"source":["from transformers import AutoModelForTokenClassification, AutoTokenizer, DataCollatorForTokenClassification\n","model = AutoModelForTokenClassification.from_pretrained(\"/kaggle/input/distilbert-all-data/giannilbert/giannilbert\")\n","tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/distilbert-all-data/giannitokenizer/giannitokenizer')\n","data_collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of = 16)"]},{"cell_type":"markdown","metadata":{},"source":["## Import test dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-03T11:17:02.038705Z","iopub.status.busy":"2024-03-03T11:17:02.038393Z","iopub.status.idle":"2024-03-03T11:17:02.045453Z","shell.execute_reply":"2024-03-03T11:17:02.044677Z","shell.execute_reply.started":"2024-03-03T11:17:02.038680Z"},"trusted":true},"outputs":[],"source":["data_test = json.load(open(\"/kaggle/input/pii-detection-removal-from-educational-data/test.json\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-03T11:17:02.046877Z","iopub.status.busy":"2024-03-03T11:17:02.046631Z","iopub.status.idle":"2024-03-03T11:17:02.061276Z","shell.execute_reply":"2024-03-03T11:17:02.060433Z","shell.execute_reply.started":"2024-03-03T11:17:02.046857Z"},"trusted":true},"outputs":[],"source":["ds_original = Dataset.from_dict({\n","    \"full_text\": [x[\"full_text\"] for x in data_test],\n","    \"document\": [x[\"document\"] for x in data_test],\n","    \"tokens\": [x[\"tokens\"] for x in data_test],\n","    \"trailing_whitespace\": [x[\"trailing_whitespace\"] for x in data_test],\n","})"]},{"cell_type":"markdown","metadata":{},"source":["## Define the function to tokenize the dataset:\n","### - The text is reconstructed from the given tokens to ensure that labels correspond to the exact token (this is not strictly necessary but is to avoid any discrepancy).\n","### - token_map contains a map character <-> word_index, where word_index is a number corresponding to its position in the document (first word in the document -> 0)\n","### - The offset mapping is returned too"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-03T11:17:02.065253Z","iopub.status.busy":"2024-03-03T11:17:02.064761Z","iopub.status.idle":"2024-03-03T11:17:02.070001Z","shell.execute_reply":"2024-03-03T11:17:02.069015Z","shell.execute_reply.started":"2024-03-03T11:17:02.065226Z"},"trusted":true},"outputs":[],"source":["max_model_input_length = 512"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-03T11:17:02.071931Z","iopub.status.busy":"2024-03-03T11:17:02.071319Z","iopub.status.idle":"2024-03-03T11:17:02.080131Z","shell.execute_reply":"2024-03-03T11:17:02.079259Z","shell.execute_reply.started":"2024-03-03T11:17:02.071896Z"},"trusted":true},"outputs":[],"source":["def tokenize(example, tokenizer):\n","\n","    text = []\n","    token_map = []\n","    \n","    idx = 0\n","    for t, ws in zip(example[\"tokens\"], example[\"trailing_whitespace\"]):\n","        \n","        text.append(t)\n","        \n","        token_map.extend([idx]*len(t))\n","        \n","        if ws:\n","            text.append(\" \")\n","            token_map.append(-1)\n","            \n","        idx += 1\n","        \n","    tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, truncation=True, padding = 'max_length',max_length = 512, return_overflowing_tokens = True)\n","    \n","    return {\n","        **tokenized,\n","        \"token_map\": token_map,\n","    }"]},{"cell_type":"markdown","metadata":{},"source":["## Tokenize the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-03T11:17:02.081666Z","iopub.status.busy":"2024-03-03T11:17:02.081319Z","iopub.status.idle":"2024-03-03T11:17:02.817508Z","shell.execute_reply":"2024-03-03T11:17:02.816310Z","shell.execute_reply.started":"2024-03-03T11:17:02.081633Z"},"trusted":true},"outputs":[],"source":["ds_original = ds_original.map(tokenize, fn_kwargs={\"tokenizer\": tokenizer}, num_proc = 4)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-03T11:17:02.819509Z","iopub.status.busy":"2024-03-03T11:17:02.819147Z","iopub.status.idle":"2024-03-03T11:17:02.827076Z","shell.execute_reply":"2024-03-03T11:17:02.826234Z","shell.execute_reply.started":"2024-03-03T11:17:02.819471Z"},"trusted":true},"outputs":[],"source":["def find_first_zero_index(lst):\n","    try:\n","        return lst.index(0)\n","    except ValueError:\n","        return -1"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-03T11:17:02.835857Z","iopub.status.busy":"2024-03-03T11:17:02.835601Z","iopub.status.idle":"2024-03-03T11:17:02.848998Z","shell.execute_reply":"2024-03-03T11:17:02.848296Z","shell.execute_reply.started":"2024-03-03T11:17:02.835836Z"},"trusted":true},"outputs":[],"source":["def set_last_sequence(row):\n","    n_seq = len(row['input_ids'])\n","    \n","    token_pos_doc = np.array([[-1] * max_model_input_length] * n_seq)\n","    for seq in range(n_seq):\n","        token_pos_doc[seq][1:-1] = np.arange(seq*(max_model_input_length - 2), (seq+1)*(max_model_input_length - 2))\n","    row['token_pos_doc'] = token_pos_doc\n","    del token_pos_doc\n","    \n","    if n_seq >= 2:\n","        zero_index = find_first_zero_index(row['input_ids'][-1])\n","        if zero_index != -1:\n","        # fix input_ids \n","            row['input_ids'][-1][-zero_index+1:] = row['input_ids'][-1][1:zero_index]\n","            row['input_ids'][-1][1:-zero_index+1] = row['input_ids'][-2][zero_index-1: -1]\n","\n","        # fix offset_mapping\n","            row['offset_mapping'][-1][-zero_index+1:] = row['offset_mapping'][-1][1:zero_index]\n","            row['offset_mapping'][-1][1:-zero_index+1] = row['offset_mapping'][-2][zero_index-1 : -1]    \n","\n","        # fix attention_mask\n","            row['attention_mask'][-1] = [1] * max_model_input_length\n","            \n","        # fix token_pos_doc\n","            row['token_pos_doc'][-1][-zero_index:-1] = row['token_pos_doc'][-1][:zero_index-1]\n","            row['token_pos_doc'][-1][1:-zero_index+1] = row['token_pos_doc'][-2][zero_index-1 : -1]\n","            # row['token_type_ids'][-1][-zero_index+1:] = row['token_type_ids'][-1][1:zero_index]\n","            # row['token_type_ids'][-1][1:-zero_index+1] = row['token_type_ids'][-2][zero_index-1 : -1]\n","    \n","    return row "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-03T11:17:02.851205Z","iopub.status.busy":"2024-03-03T11:17:02.850229Z","iopub.status.idle":"2024-03-03T11:17:03.241162Z","shell.execute_reply":"2024-03-03T11:17:03.240166Z","shell.execute_reply.started":"2024-03-03T11:17:02.851175Z"},"trusted":true},"outputs":[],"source":["tokenized_ds_wo_predictions = ds_original.map(set_last_sequence, num_proc = 2)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-03T11:17:03.242942Z","iopub.status.busy":"2024-03-03T11:17:03.242623Z","iopub.status.idle":"2024-03-03T11:17:03.250806Z","shell.execute_reply":"2024-03-03T11:17:03.249878Z","shell.execute_reply.started":"2024-03-03T11:17:03.242914Z"},"trusted":true},"outputs":[],"source":["def mean_prediction(dataframe_wo_predictions, predictions_tensor):\n","    '''\n","    PARAMETERS:\n","        -  dataframe_wo_predictions pd.DataFrame containing one input_id for each row \n","        -  predictions_tensor a (n_documents, 512, 13) tensor containing the predictions\n","    \n","    OUTPUT:\n","        -  np.array containing the labels for a single document\n","    '''\n","    predictions_tensor = tf.math.softmax(predictions_tensor.predictions, axis=-1)\n","    predictions_array = np.concatenate(predictions_tensor)\n","    dataframe_wo_predictions['predictions'] = [row for row in predictions_array]\n","    \n","    predictions_df_grouped = dataframe_wo_predictions.groupby(['document', 'token_pos_doc'], group_keys = False)['predictions'].apply('mean').reset_index()\n","    predictions_df_grouped = predictions_df_grouped.sort_values(by=['document', 'token_pos_doc'])\n","    predictions_df_grouped = predictions_df_grouped.reset_index()\n","    \n","    predicted_labels = np.array([np.array(arr) for arr in predictions_df_grouped['predictions']])\n","\n","    \n","    return predicted_labels[1:] # no start/end special tokens\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-03T11:17:03.252386Z","iopub.status.busy":"2024-03-03T11:17:03.252088Z","iopub.status.idle":"2024-03-03T11:17:03.392826Z","shell.execute_reply":"2024-03-03T11:17:03.391915Z","shell.execute_reply.started":"2024-03-03T11:17:03.252362Z"},"trusted":true},"outputs":[],"source":["from transformers import TrainingArguments, Trainer\n","args = TrainingArguments(\n","    \".\", \n","    per_device_eval_batch_size=16, \n","    report_to=\"none\",\n",")\n","trainer = Trainer(\n","    model=model, \n","    args=args, \n","    data_collator=data_collator, \n","    tokenizer=tokenizer,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-03T11:17:03.404725Z","iopub.status.busy":"2024-03-03T11:17:03.404405Z","iopub.status.idle":"2024-03-03T11:17:05.453180Z","shell.execute_reply":"2024-03-03T11:17:05.452370Z","shell.execute_reply.started":"2024-03-03T11:17:03.404697Z"},"trusted":true},"outputs":[],"source":["n_documents = len(tokenized_ds_wo_predictions)\n","tokenized_ds_wo_predictions.set_format('pandas')\n","preds_final = []\n","\n","# predict a single document each time, in this way we are saving in RAM only one document at a time\n","# while the others stay in the disk\n","for i in range(n_documents):\n","    # getting a single document inside RAM memory\n","    df_document_i = tokenized_ds_wo_predictions[i]\n","    \n","    # input_ids is a list of lists so it needs to be exploded in order to make predictions\n","    df_document_i = df_document_i.drop(columns = ['full_text', 'tokens', 'trailing_whitespace',\n","     'offset_mapping', 'overflow_to_sample_mapping',\n","       'token_map', ])\n","    exploded_df = df_document_i.explode(['input_ids','attention_mask', 'token_pos_doc',\n","#                                          'token_type_ids'\n","                                        ])\n","    \n","    predictions_document_i = trainer.predict(Dataset.from_pandas(exploded_df))\n","    \n","    # we need to explode again in order to make each row corresponding to a single input_id (token)\n","    exploded_df = exploded_df.drop(columns = ['attention_mask'])\n","    exploded_df = exploded_df.explode(['input_ids', 'token_pos_doc'])\n","    predictions_document_i = mean_prediction(exploded_df, predictions_document_i)\n","    \n","    preds_final.append(predictions_document_i)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-03T11:17:05.461691Z","iopub.status.busy":"2024-03-03T11:17:05.461430Z","iopub.status.idle":"2024-03-03T11:17:05.476529Z","shell.execute_reply":"2024-03-03T11:17:05.475600Z","shell.execute_reply.started":"2024-03-03T11:17:05.461668Z"},"trusted":true},"outputs":[],"source":["import json\n","from pathlib import Path\n","\n","config = json.load(open(Path('/kaggle/input/distilbert-all-data/giannilbert/giannilbert') / \"config.json\"))\n","id2label = config[\"id2label\"]  "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-03T11:17:05.478087Z","iopub.status.busy":"2024-03-03T11:17:05.477787Z","iopub.status.idle":"2024-03-03T11:17:05.671692Z","shell.execute_reply":"2024-03-03T11:17:05.671005Z","shell.execute_reply.started":"2024-03-03T11:17:05.478055Z"},"trusted":true},"outputs":[],"source":["def fix_offset_mapping(doc):\n","    '''Remove special tokens (start/end of sequence) from offset mapping column'''\n","    new_offset = []\n","    for seq_offset in doc['offset_mapping']:\n","        reduced_seq_offset = seq_offset[1:-1]\n","        new_offset.extend(reduced_seq_offset)\n","    doc['offset_mapping'] = new_offset\n","    return doc\n","\n","ds_original = ds_original.map(fix_offset_mapping)\n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-03T11:17:05.672870Z","iopub.status.busy":"2024-03-03T11:17:05.672634Z","iopub.status.idle":"2024-03-03T11:17:05.677093Z","shell.execute_reply":"2024-03-03T11:17:05.676206Z","shell.execute_reply.started":"2024-03-03T11:17:05.672849Z"},"trusted":true},"outputs":[],"source":["# Choose the aggregation strategy in case of conflict:\n","#     -'first': the prediction for the first subtoken decides how the whole tkoen is classified.\n","#     -'max': the prediction of the subtoken whose confidence is the highest decides how the whole tkoen is classified.\n","#     -'average': the prediction for the whole is decided by performing the mean of the predictions for all the subtokens\n","aggregation_strategy = 'average'"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-03T11:17:05.678665Z","iopub.status.busy":"2024-03-03T11:17:05.678347Z","iopub.status.idle":"2024-03-03T11:17:05.684858Z","shell.execute_reply":"2024-03-03T11:17:05.684024Z","shell.execute_reply.started":"2024-03-03T11:17:05.678637Z"},"trusted":true},"outputs":[],"source":["def max_prob_vct(lists):\n","    max_list = max(lists, key=max)\n","    return max_list"]},{"cell_type":"markdown","metadata":{},"source":["# Set the threshold and make predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-03T11:17:05.686126Z","iopub.status.busy":"2024-03-03T11:17:05.685842Z","iopub.status.idle":"2024-03-03T11:17:05.693285Z","shell.execute_reply":"2024-03-03T11:17:05.692497Z","shell.execute_reply.started":"2024-03-03T11:17:05.686104Z"},"trusted":true},"outputs":[],"source":["threshold =  0.9998688828828829"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-03T11:17:05.695179Z","iopub.status.busy":"2024-03-03T11:17:05.694583Z","iopub.status.idle":"2024-03-03T11:17:05.986747Z","shell.execute_reply":"2024-03-03T11:17:05.985744Z","shell.execute_reply.started":"2024-03-03T11:17:05.695154Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","\n","all_documents_predictions_df = pd.DataFrame(columns = ['document', 'label', 'token','token_str', 'prob'])\n","\n","# Iterate over the documents (each row in the original_ds is a distinct document)\n","for p, token_map, offsets, tokens, doc in zip(preds_final, \n","                                              ds_original[\"token_map\"], \n","                                              ds_original[\"offset_mapping\"], \n","                                              ds_original[\"tokens\"], \n","                                              ds_original[\"document\"]):\n","    \n","    document_predictions_df = pd.DataFrame(columns = ['document', 'label', 'token','token_str', 'prob'])\n","\n","    # Iterate through each token prediction in the document\n","    for token_pred, (start_idx, end_idx) in zip(p, offsets):\n","        label_pred = id2label[str(np.argmax(np.array(token_pred)))]\n","        \n","        # If start and end indices sum to zero (special token), continue to the next iteration\n","        # Since we have removed special tokens for beginning/end of a sequence, we should never\n","        # enter this.\n","        if start_idx + end_idx == 0:\n","            continue\n","\n","        # If the token mapping at the start index is a whitespace (-1), increment start index\n","        if token_map[start_idx] == -1:\n","            start_idx += 1\n","\n","        # Ignore leading whitespace tokens (\"\\n\\n\")\n","        while start_idx < len(token_map) and tokens[token_map[start_idx]].isspace():\n","            start_idx += 1\n","\n","        # If start index exceeds the length of token mapping, break the loop (end of document)\n","        if start_idx >= len(token_map):\n","            break\n","\n","        token_id = token_map[start_idx]  # relative original token\n","#         keep = label_pred != 'O'\n","        keep = token_pred[-1] < 0.99999\n","        # Ignore whitespace tokens\n","        if token_id != -1 and keep:\n","            new_prediction = [ doc,label_pred, token_id,  tokens[token_id], token_pred]\n","\n","            document_predictions_df.loc[len(document_predictions_df)] = new_prediction\n","\n","    \n","    if aggregation_strategy == 'first':\n","        document_predictions_df = document_predictions_df.groupby(['document','token','token_str'], group_keys = False, sort = False).first().reset_index()\n","    elif aggregation_strategy == 'average':\n","        document_predictions_df = document_predictions_df.groupby(['document','token','token_str'], group_keys = False, sort = False)['prob'].mean().reset_index()\n","    elif aggregation_strategy == 'max':\n","        document_predictions_df = document_predictions_df.groupby(['document','token','token_str'], group_keys = False, sort = False)['prob'].agg(max_prob_vct).reset_index()\n","    all_documents_predictions_df = pd.concat([all_documents_predictions_df, document_predictions_df], ignore_index=True)\n","    \n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-03T11:17:05.988521Z","iopub.status.busy":"2024-03-03T11:17:05.988154Z","iopub.status.idle":"2024-03-03T11:17:06.001387Z","shell.execute_reply":"2024-03-03T11:17:06.000427Z","shell.execute_reply.started":"2024-03-03T11:17:05.988489Z"},"trusted":true},"outputs":[],"source":["matrix_probs = np.array([np.array(row) for row in all_documents_predictions_df['prob']])\n","preds_normal = np.argmax(matrix_probs,axis = -1)\n","preds_without_O = np.argmax(matrix_probs[:,:12],axis = -1)\n","O_preds = matrix_probs[:,-1]\n","labels = np.where(O_preds < threshold, preds_without_O , preds_normal)\n","labels = np.array([id2label[str(label)] for label in labels])\n","all_documents_predictions_df['label'] = labels\n","all_documents_predictions_df = all_documents_predictions_df.drop(columns = ['prob'])\n","all_documents_predictions_df = all_documents_predictions_df[all_documents_predictions_df['label'] != 'O']\n","all_documents_predictions_df = all_documents_predictions_df.reset_index()\n","all_documents_predictions_df = all_documents_predictions_df.drop(columns = ['index'])\n","all_documents_predictions_df['row_id'] = all_documents_predictions_df.index"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-03T11:17:06.003018Z","iopub.status.busy":"2024-03-03T11:17:06.002694Z","iopub.status.idle":"2024-03-03T11:17:06.025946Z","shell.execute_reply":"2024-03-03T11:17:06.024985Z","shell.execute_reply.started":"2024-03-03T11:17:06.002990Z"},"trusted":true},"outputs":[],"source":["all_documents_predictions_df"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-03T11:17:06.027799Z","iopub.status.busy":"2024-03-03T11:17:06.027323Z","iopub.status.idle":"2024-03-03T11:17:06.186713Z","shell.execute_reply":"2024-03-03T11:17:06.185544Z","shell.execute_reply.started":"2024-03-03T11:17:06.027767Z"},"trusted":true},"outputs":[],"source":["all_documents_predictions_df.to_csv(\"submission.csv\", index=False)"]}],"metadata":{"colab":{"provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":7500999,"sourceId":66653,"sourceType":"competition"},{"datasetId":4421492,"sourceId":7596123,"sourceType":"datasetVersion"},{"datasetId":4523606,"sourceId":7739593,"sourceType":"datasetVersion"},{"datasetId":4530685,"sourceId":7749663,"sourceType":"datasetVersion"},{"datasetId":4532275,"sourceId":7751700,"sourceType":"datasetVersion"},{"datasetId":4533522,"sourceId":7753455,"sourceType":"datasetVersion"},{"datasetId":4533547,"sourceId":7753484,"sourceType":"datasetVersion"}],"dockerImageVersionId":30646,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
